<style type="text/css">
    body {
        font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
        font-weight:300;
        font-size:18px;
        margin-left: auto;
        margin-right: auto;
        width: 1100px;
    }

    h1 {
        font-weight:300;
    }

    .disclaimerbox {
        background-color: #eee;
        border: 1px solid #eeeeee;
        border-radius: 10px ;
        -moz-border-radius: 10px ;
        -webkit-border-radius: 10px ;
        padding: 20px;
    }

    video.header-vid {
        height: 140px;
        border: 1px solid black;
        border-radius: 10px ;
        -moz-border-radius: 10px ;
        -webkit-border-radius: 10px ;
    }

    img.header-img {
        height: 140px;
        border: 1px solid black;
        border-radius: 10px ;
        -moz-border-radius: 10px ;
        -webkit-border-radius: 10px ;
    }

    img.rounded {
        border: 1px solid #eeeeee;
        border-radius: 10px ;
        -moz-border-radius: 10px ;
        -webkit-border-radius: 10px ;
    }

    a:link,a:visited
    {
        color: #1367a7;
        text-decoration: none;
    }
    a:hover {
        color: #208799;
    }

    td.dl-link {
        height: 160px;
        text-align: center;
        font-size: 22px;
    }

    .layered-paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
        box-shadow:
                0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
                5px 5px 0 0px #fff, /* The second layer */
                5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
                10px 10px 0 0px #fff, /* The third layer */
                10px 10px 1px 1px rgba(0,0,0,0.35), /* The third layer shadow */
                15px 15px 0 0px #fff, /* The fourth layer */
                15px 15px 1px 1px rgba(0,0,0,0.35), /* The fourth layer shadow */
                20px 20px 0 0px #fff, /* The fifth layer */
                20px 20px 1px 1px rgba(0,0,0,0.35), /* The fifth layer shadow */
                25px 25px 0 0px #fff, /* The fifth layer */
                25px 25px 1px 1px rgba(0,0,0,0.35); /* The fifth layer shadow */
        margin-left: 10px;
        margin-right: 45px;
    }


    .layered-paper { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
        box-shadow:
                0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
                5px 5px 0 0px #fff, /* The second layer */
                5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
                10px 10px 0 0px #fff, /* The third layer */
                10px 10px 1px 1px rgba(0,0,0,0.35); /* The third layer shadow */
        margin-top: 5px;
        margin-left: 10px;
        margin-right: 30px;
        margin-bottom: 5px;
    }

    .vert-cent {
        position: relative;
        top: 50%;
        transform: translateY(-50%);
    }

    hr
    {
        border: 0;
        height: 1.5px;
        background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
    }
</style>


  
        <title>IQ-FlOW</title>
        <meta property="og:title" content="MVC">
  </head>

  <body data-new-gr-c-s-check-loaded="14.991.0" data-gr-ext-installed="">
    <br>
    <center>
    <span style="font-size:42px">IQ-Flow: Mechanism Design for Inducing Cooperative Behavior to Self-Interested Agents in Sequential Social Dilemmas</span>
    </center>

    
      
      <table align="center" width="700px">
       <tbody><tr>
        <td align="center" width="100px">
        <center>
        <span style="font-size:20px">Bengisu Guresti</sup></span>
        </center>
        </td>
        <td align="center" width="100px">
        <center>
        <span style="font-size:20px">Abdullah Vanlioglu</sup></span>
        </center>
        </td>
        <td align="center" width="100px">
        <center>
        <span style="font-size:20px"><a href="http://kemalure.com/"> Nazim Kemal Ure</sup></a></span>
        </center>
        </td>

     
     </tr></tbody>
            <br>  
            <center>
                <span style="font-size:28px">&nbsp;<a href="https://www.ifaamas.org/Proceedings/aamas2023/pdfs/p2143.pdf"> Paper</a></span>
                <br>
                
                <br>
             <center><h1>Abstract</h1></center><table align="center" width="900px">
                <br><br>
                <div align="justify">
                    Achieving and maintaining cooperation between agents to accomplish a common objective 
                    is one of the central goals of Multi-Agent Reinforcement Learning (MARL). 
                    Nevertheless in many real-world scenarios, separately trained and specialized agents 
                    are deployed into a shared environment, or the environment requires multiple objectives 
                    to be achieved by different coexisting parties. These vari- ations among specialties and 
                    objectives are likely to cause mixed motives that eventually result in a social dilemma where 
                    all the parties are at a loss. In order to resolve this issue, we propose the Incentive Q-Flow (IQ-Flow) 
                    algorithm, which modifies the system’s reward setup with an incentive regulator agent such that the 
                    cooperative policy also corresponds to the self-interested policy for the agents. Unlike the existing 
                    methods that learn to incentivize self-interested agents, IQ-Flow does not make any assumptions about 
                    agents’ policies or learning algorithms, which enables the generalization of the developed framework to 
                    a wider array of ap- plications. IQ-Flow performs an offline evaluation of the optimality of the learned 
                    policies using the data provided by other agents to de- termine cooperative and self-interested policies. 
                    Next, IQ-Flow uses meta-gradient learning to estimate how policy evaluation changes according to given 
                    incentives and modifies the incentive such that the greedy policy for cooperative objective and self-interested 
                    objective yield the same actions. We present the operational characteristics of IQ-Flow in Iterated Matrix Games. 
                    We demonstrate that IQ-Flow outperforms the state-of-the-art incentive design al- gorithm in Escape Room and 
                    2-Player Cleanup environments. We further demonstrate that the pretrained IQ-Flow mechanism significantly 
                    outperforms the performance of the shared reward setup in the 2-Player Cleanup environment.</div>
                    <br><br>
                <tbody><tr>
                </tr>
                    <tr><td width="600px">
                      <center>
                          <span style="font-size:14px"><i> </i>
                    </span></center>
                    </td>
                </tr>
            </tbody></table>

            <br><br>
            <br><br>

          <hr>


          <center><h1>Presentation</h1></center>
          <center> 
            <iframe id="fred" style="border:1px solid #666CCC" title="aamas_presentation" src="aamas_presentation.pdf" frameborder="1" scrolling="auto" height="800" width="1000" ></iframe>

        </center>
        <br><br>
        <br><br>
           <hr>

            <table align="center" width="1100px">
                <tbody><tr>
                    <td>
                      <left>
                <center><h1>Acknowledgements</h1></center>
                This webpage template was borrowed from <a href="https://richzhang.github.io/colorization/">colorful folks</a>.
            </left>
        </td>
        </tr>
        </tbody></table>

        <br><br>


</body></html>
